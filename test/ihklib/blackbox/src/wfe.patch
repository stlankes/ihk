diff --git a/arch/arm64/kernel/include/arch-lock.h b/arch/arm64/kernel/include/arch-lock.h
index a9f7c339..5ec4541c 100644
--- a/arch/arm64/kernel/include/arch-lock.h
+++ b/arch/arm64/kernel/include/arch-lock.h
@@ -173,7 +173,8 @@ static void __ihk_mc_spinlock_lock_noirq(ihk_spinlock_t *lock)
 	 * unlock before the exclusive load.
 	 */
 "	sevl\n"
-"2:	wfe\n"
+/*"2:	wfe\n"*/
+"2:	nop\n"
 "	ldaxrh	%w2, %4\n"
 "	eor	%w1, %w2, %w0, lsr #16\n"
 "	cbnz	%w1, 2b\n"
@@ -674,7 +675,8 @@ static inline void ihk_mc_read_lock(struct ihk_rwlock *rw)
 
 	asm volatile(
 	"       sevl\n"
-	"1:     wfe\n"
+	/*"1:     wfe\n"*/
+	"1:     nop\n"
 	"2:     ldaxr   %w0, %2\n"
 	"       add     %w0, %w0, #1\n"
 	"       tbnz    %w0, #31, 1b\n"
@@ -722,7 +724,8 @@ static inline void ihk_mc_write_lock(struct ihk_rwlock *rw)
 
 	asm volatile(
 	"       sevl\n"
-	"1:     wfe\n"
+	/*"1:     wfe\n"*/
+	"1:     nop\n"
 	"2:     ldaxr   %w0, %1\n"
 	"       cbnz    %w0, 1b\n"
 	"       stxr    %w0, %w2, %1\n"
